+++ 
draft = false
date = 2019-01-14T15:35:50+08:00
title = "[Review] Regression"
slug = "[Review] Regression" 
tags = ["Review"]
categories = ["Machine Learning"]
+++
# 线性回归


## 1. 回归模型

回归任务主要是处理连续值的模型。回归学习是用来估计输入值和输出值之间的关系，建立输入值和输出值之间的数学模型，当再来一个新样本时，可以预测新样本的输出值。回归是一个有监督的学习过程，在建立输入值和输出值之间的数学模型时，需要有训练集，在预测的时候需要一些先验知识。

典型例子：房价的估计

### 分类

**1. 按照输入值的数量**

+ 单元回归，输入的特征值只有一个
+ 多元回归，输入多个特征值

**2. 输入与输出的关系**

+ 线性回归
+ 非线性回归

基本模型：
$$
f(x_i) = wx_i  + b
$$
最常用的性能度量是均方误差，可以试图通过令均方误差最小化求得参数。

## 2.最小二乘法

### 2.1 单元线性回归

对于只有一维特征的线性回归，可以直接使用均方误差作为损失函数。同时，对于$w, b$求偏导。令其等于0，即可以求出模型的两个参数。

### 2.2 多元线性回归

更一般的情形是对于一个数据集D由d个属性进行描述，所以输入的特征是d维特征，此时试图学得的模型可以表示为：
$$
f(x_i) = w^T x_i + b
$$
可以把参数$w^T ,b吸收入向量的形式 \hat{w} = (w^T; b)$,同时把数据集D表示为一个`m * (d+1)` 的矩阵形式，其中每一行代表一个样本，每一列代表一个属性，同时把标记也写作向量形式 `m * 1` 

**优化目标：**
$$
\hat{w} = argmin _{ \hat{w} } (y-X \hat {w})^T (y - X \hat {w}) 
\\\\ E _{ \hat {w}} = (y-X \hat {w}) ^T(y - X \hat {w})
$$
**求偏导：**
$$
\frac{ \partial E}{\partial \hat{w}} = 2 X^T(X \hat{w} - y)
$$
令上式等于0可以得到参数的最优解，但是由于涉及矩阵的逆，所以需要进行讨论。如果，$X^TX$是一个满秩矩阵或者正定矩阵，可以直接求：
$$
\hat{w}^* = (X^TX)^{-1}X^Ty
$$
但现实任务往往不是满秩矩阵（没有一行或者一列为０），就会有多个解，这是需要使用正则化的方法。使用正则化技术，减少特征前面的参数w的大小，具体就是修改线性回归中的损失函数形式即可，岭回归以及Lasso回归就是这么做的。或者丢弃一些对我们最终预测结果影响不大的特征，具体哪些特征需要丢弃可以通过PCA算法来实现；	

## 3.最小二乘法的缺点

+ 模型可能过拟合
+ 在求解中需要对矩阵进行转置，但是非满秩矩阵的转置不存在，存在多解

## 4. 范数Norm

![norm](http://media.innohub.top/190114-norm.png)



## 5. 岭回归，Lasso回归

ridge 回归和lasso回归可以防止模型过拟合，也可以解决样本的特征数远超过样本数的问题。分别在初始线性回归模型上加`l-2` 范数，`l-1` 范数

### 5.1 Ridge Regression

$\lambda$是需要设定的超参数。通常选取在验证集上损失最小的超参数。

**损失函数：**
$$
E_{\hat{w}} = (y-X \hat{w})^T(y - X\hat{w}) + \lambda||\hat{w}||^2  \qquad \lambda >0
$$
**求偏导：**
$$
\frac{ \partial E}{\partial \hat{w}} = 2 X^T(X \hat{w} - y) + \lambda I \hat{w}, \qquad  I:Identity Matrix
$$
**解析解：**
$$
\hat{w}^* = (X^TX + \lambda I)^{-1}X^Ty
$$
对比一下普通的线性回归的解析解和岭回归的解析解，可以发现需要求逆的那一项变化了，岭回归解析解中求逆那一项在x转置x的基础上加了一项单位矩阵，在加入单位矩阵后就可以使得需要求逆那一项是满秩矩阵，不会出现不满秩的情况无论x转置x是满秩还是非满秩，x转置x+I都是满秩，可以解决最小二乘法作为损失函数所不能解决的特征数大于样本数的问题

### 5.2 Lasso Regression

**损失函数：**
$$
E_{\hat{w}} = (y-X \hat{w})^T(y - X\hat{w}) + \lambda||\hat{w}||_1  \qquad \lambda >0
$$
**在求解lasso回归的$\hat{w}$时要注意，不能用损失函数对其求导，因为一范数在0点处导数不存在。以y=|w|为例，w属于实数范围，在w=0点处，y的左导数是-1，右导数是1，左右导数不想等，所以不能对lasso回归的损失函数求导**

 **使用proximal gradient descent method(近端梯度下降法)** 

使用lasso回归的主要特点在于解的大部分元素是0，而以此作为参数进行线性回归，对于为0的特征进行了忽略，所以可以进行特征选择，选出重要特征。

## 6. 广义线性回归

### 6.1 基本形式

对于任意的单调可微函数$g$,广义线性回归的一般形式：

$f(x ) = g^{-1}(w^Tx +b)$

### 6.2 分类任务--对数几率回归（极大似然法MLE）

将线性回归用于分类任务，只需要确定合适的阶跃函数，预测值位于某个区间时输出为某种标记。单位阶跃函数不连续，可以使用**对数几率函数（Logistic Function）**替代
$$
g(z) = \frac{1}{1+e^{-z}}
$$
**目标函数：**
$$
y = \frac{1}{1 + e^{-(w^Tx +b)}}
$$
**对数几率：**
$$
ln \frac{y}{1-y} = w^Tx+b  \qquad (1)
$$
其中$ln \frac{y}{1-y}$ 被称为对数几率，y代表了样本为正例的可能，1-y代表为负例的可能，则该比值代表了样本作为正例的相对可能性。

**如果将y视为类别的后验概率估计，则(1)可以改写为：**
$$
ln \frac{p(y = 1 | x)}{p(y = 0 | x)} =w^Tx+b  \qquad (2)
\\\\ 显然有：
\\\\ p(y = 1 | x) =\frac{e^{(w^Tx +b)}}{1 + e^{(w^Tx +b)}}
\\\\ p(y = 0 | x) =\frac{1}{1 + e^{(w^Tx +b)}}
$$
所以可以使用**MLE**来估计参数。
$$
l(w, b) = \sum _{i=1}^m p(y_i \mid x_i; w, b)
$$


### 6.3 对数几率回归的特点

+ 无需事先假设数据分布
+ 可以得到“类别”的近似概率预测
+ 可直接应用优化算法求解

## 7. 多分类任务

对于多分类任务，可以直接使用二分类的方法进行推广。基本思路是使用“拆解法”，将多分类任务拆分为多个二分类任务。

### 拆分策略

+ 一对一
+ 一对其他
+ 多对多

![to1](http://media.innohub.top/190114-to1.png)

![ton](http://media.innohub.top/190114-ton.png)

![com](http://media.innohub.top/190114-com.png)

### 类别不平衡

+ 欠采样：去除一些反例，使得正方数目接近
+ 过采样：增加正例
+ 阈值移动：
