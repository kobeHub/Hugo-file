+++
draft = false
date = 2019-01-13T17:11:19+08:00
title = "[Review] Bayesian decision theory"
slug = "[Review] Bayesian decision theory"
tags = ["Review"]
categories = ["Machine Learning"]
+++
# Bayesian decision theory



## 1. 常用的贝叶斯决策规则

+ 基于最小错误率的贝叶斯决策
+ 基于最小风险的贝叶斯决策
+ 在限定一类错误率条件下使另一类错误率最小的两类别决策
+ 最小最大决策

## 2.基本概率公式

+ **条件概率：**$p(A|B)$表示事件A在事件B发生的条件下发生的概率

+ **联合概率：**$p(A. B)$ 表示两个事件同时发生的概率，当两个随机事件相互独立时，联合概率等于概率的乘积

  **基本条件概率公式：**
  $$
  P(A | B) = \frac{P(A, B)}{P(B)}
  $$
  所以当A B相互独立时，$P(A|B) =  P(A)$ .

+ **全概率公式：**  如果B是由相互独立的事假组成的概率空间 $\{B_1, B_2,...,B_n\}$, 那么关于事件A的全概率公式可以写作：

  $$P(A) = \sum_{i=1}^{n}P(B_i)P(A|B_i)$$

  对于全概率公式可以通过，条件概率进行推导，相当于将所有在B的子事件和同时A发生的概率的和，因为B的概率空间之和为1，所以最终得到的是A发生的概率。

+ **贝叶斯公式：**

  $$
  P(B_i | A) = \frac{P(B_i)P(A|B_i)}{\sum _{i=1}^{n}P(B_i) P(A|B_i) }
  $$


  其中 $P(B_i|A)$ 被称为后验概率，$P(B_i)$ 为先验概率，$P(A|B_i)$ 是条件概率



## 3. 基于最小错误率的贝叶斯决策

模式分类问题中，基于减少分类错误的要求。使用概率论中的贝叶斯公式，可得出使得分类错误率规则。

以简单的二分类任务为例，类别的状态用变量$w$表示，其中$w_1$表示正常，$w_2$表示异常。$p(w_1), p(w_2)$是已知的先验概率。那么有：

+ $p(x|w_1):$样本x为正常类的条件概率密度
+ $p(x|w_2):$样本x为正常类的条件概率密度

使用贝叶斯公式，给定一个样本x，可以求出所属类的后验概率，然后选取后验概率更大的一类作为x的分类标签。

$$
P(w_i | x) = \frac{P(x|w_i)P(w_i)}{\sum _{j=1}^2P(x|w_j)P(w_j)}
$$

分类规则可以写作：

$$
P(w_i|x) = max _{j = 1,2} P(w_j| x)
$$

使用这种决策方式，决策结果仅取决于实际观察到的**类条件概率密度**以及**先验概率**，如果先验概率有明显的偏向性，那么决策结果也有明显的偏向性。

### 平均错误率最小的证明：

## 4. 基于最小风险的贝叶斯决策

### 1. 基本概念

使得分类错误率达到最小是必要的，但是对于实际问题，有时需要考虑另一个比错误率更加广泛的概念----**风险**。风险与损失是紧密相连的。

例如癌症细胞的判别，将异常细胞判别为正常的风险显然比将正常判别为异常的风险大。基于决策论的角度进行考虑。在决策轮中，采取的决定称为**决策**或者**行动**，所有可能采取的决策组成的集合称之为**决策空间**，**行动空间**，用A表示。而每一个决策都会带来一定的损失。**状态空间**又称为**自然空间**。

### 2. 形式化表示

+ 对于观察X是一个d维的随机向量：$X = [x_1, x_2,...,x_d]^T$

+ 状态空间$\omega$ 是由c个自然状态组成（C类）
  $$
  \omega = \{\omega_1,\omega_2,...,\omega_c\}
  $$

+ 决策空间由a个决策组成$a_1,..a_a$，**一般而言c类对应于c个决策，但是可能存在“拒绝”的情形，所以使用a个决策**

为了引入损失的概念，对于给定的x，如果采取的决策从决策表可见，那么对应于决策$a_i$,可以在c个类中任取一个，对应的损失为 $\lambda(a_i, \omega_j)$ ,而取得该类的**后验概率**为$P(w_j|x)$.那么就可以得到条件期望损失：
$$
R(a_i | x) = \sum _{j=1}^c\lambda(a_i, \omega_j)P(w_j|x)
$$
条件期望损失又被称为条件风险，对于条件风险进行积分，可以得到期望风险R。

>**期望风险**反映了对于整个特征空间上的所有x取值采取相应的决策$a_i$,所带来的平均风险；而**条件风险**反映的是对于某一具体的x采取决策带来的风险。
>
>在考虑错判带来的损失时，我们希望期望风险最小；如果在对每一个决策采取行动时，都使其条件风险最小，那么期望风险一定也最小。这样的决策就是最小风险贝叶斯决策。

### 3. 决策规则

+ 利用已知的先验概率$P(w_j)$,条件概率$P(x|w_j), j=1,2...,c$   以及给出待识别的`x` 等条件下。计算后验概率$P(w_i| x), i=1,2...,c$
+ 利用计算得到的后验概率以及决策表，计算采用每一种决策$a_i$的**条件风险**,  $R(a_i | x)$
+ 找出风险最小的决策作为最终决策

### 4. 0-1损失函数的特殊情形

基于最小风险的贝叶斯决策，不仅需要符合实际的先验概率以及类条件概率，还需要合适的损失函数，一般需要使用专业领域知识确定。

在不考虑“拒绝”的决策时，如果使用最为简单`0-1损失函数`，那么这时的最小风险决策就是最小错误率的贝叶斯决策。后者是前者的一个特殊形式。

```
0-1损失： 决策与状态下表相等时损失0；不等时损失1
```

## 5. 限定一类错误率条件下使另一类错误率最小的两类别决策

由于先验概率对于具体问题而言，通常是确定的。所以一般称$p_1(e), p_2(e)$为两类错误率。实际使用中，有时要求限制其中一类的错误率不得大于某个常数，使得另一类错误率尽可能小。

## 6. 最大最小决策

对于给定的x其先验概率不变，一般通过贝叶斯规则，可以使得错误率或者风险最小。但是对于未知的先验概率或者是可变的先验概率，往往无法得到结果。

最大最小决策就是考虑先验概率可变时，如何使得最大可能风险最小，在最差情况下争取最好结果。是一种偏于保守的分类方法。

# 类条件概率密度的估计

## 1. 主要问题

由于在实际中，从有限的样本集中可能未知两个重要的前提条件，类条件概率密度以及先验概率。所以需要使用样本进行估计；同时需要探讨估计量的性质；利用样本集后估计错误率的方法。

**从样本推断总体概率分布的方法：**

+  监督参数估计：样本所属类别以及总体的概率密度函数形式已知，而某些参数未知。**MLE，贝叶斯估计**
+ 非监督参数估计：已知总体的概率密度形式，但是样本无标记，求解概率密度函数的参数
+ 非参数估计：对于有标记数据，不知道数据的概率密度函数形式 **Parzen**， **KN近邻**

## 2. 最大似然估计（MLE）

### 2.1 假设

最大似然估计基于以下假设：

+ 参数$\theta$是确定而未知的
+ 样本与总体独立同分布，样本独立抽取
+ 类条件概率密度具有某种特定函数形式
+ 不同类别的参数在函数上是独立的，每一类别的样本只对本类估计有影响

### 2.2 定义

对于某一类样本的集合的N个样本，$X = \{x_1, x_2, ..., x_n\}$, 假设样本是独立抽取的，那么似然函数如下：

$$P(X|\theta) = P(x_1, x_2, ...,x_n) = \prod _{k=1}^nP(x_k|\theta)
$$

叫做参数$\theta$相对于样本集X的似然函数。给出了从总体中抽出这n个样本的概率。**如果$\theta$是已知的，那么哪一个样本集被抽取的可能性最大呢？显然是令该似然函数值最大的样本集被抽取的概率最大。**

那么根据此思路，当 参数未知，根据给定的样本集，我们需要找到其来自哪一个密度函数的概率最大，就是要在参数空间找一个值$\theta$ 使得似然函数最大。

### 2.3 求解

对于似然函数的连乘可能造成下溢，通常使用对数似然。
$$
H(\theta) = ln(l(\theta))
$$
然后对$\theta$求出梯度算子，令其等于0，计算使得目标函数为最大值的参数值**作为概率密度分布函数的参数**。

## 3. 贝叶斯估计

贝叶斯决策与贝叶斯估计两者都是立足于使得贝叶斯风险最小，只是解决的问题不同，前者**前者是要决策x的真实状态，而后者是估计样本集X所属总体分布的参数。**

![baye](https://mediainter.innohub.top/190113-baye.png)

|          | 贝叶斯决策          | 贝叶斯估计                 |
| -------- | ------------------- | -------------------------- |
| 问题性质 | **决策问题**        | **估计问题**               |
| 已知     | 样本x               | 样本集X                    |
| 需要     | 做出决策$a_i$       | 得出估计量$\theta^*$       |
| 客观存在 | 真实状态$w_j$       | 真实参数$\theta$           |
| 已知     | 状态空间A是离散空间 | 参数空间$\theta$是连续空间 |
| 已知     | 先验概率$p(w_j)$    | 待估计参数的分布形式       |



## 4. 非参数估计任务

​      以上讨论的参数估计方法要求已知总体分布的形式。然而许多实际问题并不知道总体分布形式，或总体分布形式不是一些通常遇到的典型分布，不能写成某些参数的函数。为了设计贝叶斯分类器，仍然需要总体分布的知识，于是提出了某些直接用样本来估计总体分布的方法，称为估计分布的非参数法。

​    **估计总体分布密度函数有很多方法，它们的最根本出发点是随机向量x落入区域R的概率P。**对于给定的样本，已知其类别。对于一个以为的样本而言，假定所有的样本位于一个范围[a, b]之间，我们要求的的东西就是，给定一个区间内的值，求出该值的概率。也就是求每一点的密度大小。

![no arg](https://mediainter.innohub.top/190113-noarg.png)

​       要估计无限个点，任何一个点，找一个邻域，一个框，看它框住几个样本点，如果框住的样本点越多说明这个地方估计值越高，框出的样本点越少说明这个地方密度越小，这样的话，这个公式就很好理解了，那个大N是没有用的，这个大N除不除是没有用的，大N都扔了也没有关系，因为对于同一组系数来说，N都是一样的，就是一个因子分子，有用的是KN和V，V这个就是框是多大，框是一定的话，V就定了，那么就是KN就是圈几个点，圈的点越多，这个点的密度就越大，圈的点越少，这个点密度就越小，根据公式也可以计算具体的值，这个具体的值就是x,y点的密度，也就是这个空间中样本的密度，注意这个密度不一定样本点所在位置的密度，也可以是没有样本点所在位置的估计，每个有效区域上都可以，所以看这个公式非常好理解，

​      样本总数为N，在每个区域中，体积为V时所包含的的样本数Kn。在求x处的密度估计p时，我们得到包含x的区域体积，区域内恰好落入kN个样本，用这个式子来估计p。对于parzen窗，就是每个区域中的体积是固定的，我们计算估计每个区域中样本的个数。kn近邻就是样本个数已知，我们计算每个区域中包含给定样本个数的体积, 进而求概率分布形式。

## 5. Parzen窗法以及$K_N$近邻法

用来估计一点的密度的“邻域”的大小，都是根据样本集的大小来确定的。对一个固定大小的样本集，两种方法均有自己确定“邻域”大小的方法：

+ Parzen窗法：“邻域” 体积大小是固定，但“邻域”包含的样本点数目可能不同
+  近邻法： “邻域”内包含的样本点数目固定，但“邻域”的体积可能不同
