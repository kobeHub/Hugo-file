+++
draft = false
date = 2019-01-13T21:05:17+08:00
title = "[Review] Decision Tree"
slug = "[Review] Decision Tree" 
tags = ["Review"]
categories = ["Machine Learning"]
+++
# Decision Tree



## 1. 决策树的定义以及建立

决策树是一种常见的机器学习方法。以二分类问题为例，决策树模拟人类处理问题的机制，对于某一个问题进行决策时，进行一些列的判断或者子决策。对于一棵决策树来说，具有以下属性：

+ 每一个非叶结点都是一个属性的划分
+ 每一个划分指向下一个决策或者指向最终结论
+ 决策过程从根结点开始，一直到叶结点
+ 最终的结论对应一个目标值

*基本决策树算法描述：*

```markdown
1. 选择一个最优属性对于剩余样本进行划分，并且将该属性作为一个结点
2. 重复上述过程构造后代结点
3. 遇到以下条件停止：
       所有实例具有相同的值
       在所有的属性集中，没有属性或者是实例具有相同的值
       实例已全部划分
```

所以关于决策树的构建，最关键的问题在于**最优属性的选取**，不同的决策树算法实则是不同的属性选取方式

## 2. Information Gain

### 2.1 信息熵

随着划分的进行，我们希望决策树的分支结点所包含的样本尽可能属于同一个类别，即我们希望结点的“纯度”越来越高。信息熵**（Information Entropy）**可以作为纯度的一个度量标准，信息熵的计算方式：对于一个样本集合D中的第k类样本所占的比例为$p_k(k = 1,2,...,|y|)$,则该样本集合的信息熵为：
$$
Ent(D) = - \sum _{k=1}^{|y|}p_klog_2p_k
$$
信息熵越小，纯度越高。针对二分类问题，信息熵可以形式化为：$Entropy = -p_1log_2p_1-(1-p_1)log_2(1-p_1)$ 

当所有的样本属于同一个类别是，信息熵值为0.

### 2.2 信息增益

**Information Gain**可以表述为，一个通过对于一个属性的划分，原来样本集熵的下降情况。是一个属性的信息增益。所以对于一个属性而言，信息增益越大，说明使用该属性划分得到的**纯度提升越大**。

对于离散属性a，可能具有k个不同的取值$\{a^1, a^2,...,a^k\}$ ,所以对该属性进行划分会得到k个结点，将原样本集分为k个样本$D^1 ... D^k$  
$$
Gain(D, a) = Ent(D) - \sum _{i=1}^k \frac{|D^i|}{|D|}Ent(D^i)
$$
有了信息增益，每次选取信息增益最大的属性进行划分。

### 2.3 信息增益率

信息增益准则对于可取值数目较多的属性有所偏好，如果把样本的编号作为一个属性添加进去，那么一个编号只有一个样本，“纯度”最高，所以会选取该属性进行样本的划分，但是这样的决策树显然不具有泛化能力，无法进行有效预测。

所以可以使用信息增益率作为**最优属性**的选择标准：
$$
Gain\\_ ratio(D, a) = \frac{Gain(D, a)}{IV(a)}  \\\\
IV(a) = - \sum _{i=1}^k \frac{|D^i|}{|D|}log_2\frac{|D^i|}{|D|}
$$
$IV(a)$是属性a的*固有值（Intrinsic Value）* ，属性a的取值数目越多，固有值越大，从而可以用来降低信息增益对于取值数目多的属性的偏好。

## 3. Pruning 剪枝处理

剪枝处理是避免决策树过拟合的主要手段。基本策略有：

+ 前剪枝**（Forward Pruning，prepruning）**，也称之为预剪枝。
+ 后剪枝**（Backward Pruning， post-pruning）**

主要依据是决策树的泛化能力：在验证集上的准确率。前剪枝是指在决策树生成过程中，对于每一个结点划分前进行估计，如果进行划分不能带来泛化性能提升，则停止划分。后剪枝指先生成一颗完成的决策树，然后自底而上对非叶结点进行考察，如果将该结点的子树替换为叶结点可以提升泛化性能，则进行替换。

### 前剪枝的特点：

+ 前剪枝使得决策树很多分支没有展开，不仅降低了过拟合的风险，而且显著减少了决策树的训练时间开销以及测试时间开销。
+ 基于“贪心”的本质，可能带来欠拟合风险

### 后剪枝特点：

+ 后剪枝保留了更多分支，泛化能力相对更强，没有欠拟合风险。
+ 时间开销很大

## 4. 连续值处理

决策树针是对离散属性进行决策的，处理连续值时，需要将其离散化。最简单的做法是使用**二分法**，对于一个连续属性，设置一个阈值$T_a$,如果值小于阈值则为true，否则为false。

那么如何选取阈值，即划分点呢？对于具有k个值的连续属性，可以选择k-1个中位数，作为可选划分点，然后针对每一个划分点，计算其信息增益，选取信息增益最大的一个值作为划分点。

## 5. 缺失值处理

对于样本中某些属性的值缺失，进行信息增益计算时只需要进行推广计算即可。例如$\hat{D}$ 表示D中在属性a上没有缺失的样本子集，所以在计算时，可以直接使用这些样本计算信息熵：
$$
Ent(\hat{D}) = -\sum_{k=1}^{|y|} \hat{p_k}log_2 \hat{p_k}
$$
​	进而信息增益公式，需要加上所使用的样本子集的**权重**，即使用的样本子集的比例：
$$
Gain(D, a) = \rho \ast \lbrace Ent(D) - \sum _{i=1}^k \frac{|D^i|}{|D|}Ent(D^i) \rbrace
$$


## 6. 处理能力

利用决策树进行分类工作，分类边界每一段都与坐标轴平行，这样的分类使得结果具有良好的可解释性，面对较为复杂的分类任务需要很多段划分才能较好的近似。

缺点在于对于非矩形区域的划分能力不足
